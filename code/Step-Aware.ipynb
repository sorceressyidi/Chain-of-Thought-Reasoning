{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2hjmGenjlX3H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hjmGenjlX3H",
        "outputId": "d394cb75-f0ef-44c8-d13e-e002fd0a598d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "322c0b68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "322c0b68",
        "outputId": "6fc22e46-158c-43eb-ad1c-9748bc495a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['data', 'cot.ipynb', 'main.ipynb', 'aware_infer (1).ipynb', 'aware_infer.ipynb']\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'NLP_proj_lzp'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "zaTQXxW_EURA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaTQXxW_EURA",
        "outputId": "3f5deafd-17d7-43c5-b95e-a302f7b91c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['test.json', 'test.tok.json', 'train.tok.json', 'train.json', 'train.csv', 'predict.csv']\n",
            "drive/My Drive/NLP_proj_lzp/data/train.json\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data')\n",
        "print(os.listdir(DATA_PATH))\n",
        "file_name = 'train.json'\n",
        "file_path = os.path.join(DATA_PATH, file_name)\n",
        "print(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bcc0208e-3378-46e2-a8fc-beeba04ed15f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b10e37c29c5c446291cca30192fda169",
            "e9007005bb024056b8e879d34d7a1248",
            "af1bbefab10e4c5c9cb7da6a3641d58c",
            "d683fb0093cc4cd8abf62ab6e19a7068",
            "9bc883da40f449a698a2136a98bfb57d",
            "b25fc54d63ed482b9eb9fb5aaf14890c",
            "506a2d455ab745a3a26cbec84634f66f",
            "0f23a82d8a054641ae3017fe6d41eb41",
            "0043cccd25ea479c98ecf93605cc384e",
            "2b2241d4068b412fb4be01e63ea9058f",
            "4131e626015e4850aefad8ec21560ca6"
          ]
        },
        "id": "bcc0208e-3378-46e2-a8fc-beeba04ed15f",
        "outputId": "4381d96d-a498-43a6-a9c3-3b8f5e94749d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b10e37c29c5c446291cca30192fda169",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import copy\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers.generation import GenerationConfig\n",
        "import json\n",
        "import torch\n",
        "import accelerate\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.1\", device_map=device)\n",
        "model.generation_config = GenerationConfig.from_pretrained(\"lmsys/vicuna-7b-v1.1\")\n",
        "model.generation_config.do_sample = False\n",
        "\n",
        "pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "702addb7-6792-40d6-8a0d-fa2a8392ca2a",
      "metadata": {
        "id": "702addb7-6792-40d6-8a0d-fa2a8392ca2a"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Conservative Sampling\n",
        "hyperparameters_1 = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 100,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# Middleground Sampling\n",
        "hyperparameters_2 = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 100,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 80,\n",
        "    \"top_p\": 0.9,\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# Creative Sampling\n",
        "hyperparameters_3 = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 100,\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_k\": 100,\n",
        "    \"top_p\": 0.85,\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "hyperparameters_list = [hyperparameters_1, hyperparameters_2, hyperparameters_3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef657437",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef657437",
        "outputId": "3038fb56-3d34-4c52-c27d-f073145b0fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1:\n",
            "Question: Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?\n",
            "A)21\n",
            "B)21.5\n",
            "C)22\n",
            "D)22.5\n",
            "E)23\n",
            "Correct answer: E\n",
            "Question: There are k-2 members in a certain band, including Jim and Ellen. Two members are to be selected to attend the Grammy awards ceremony. If there are 6 possible combinations in which Jim and Ellen are not selected, what is the value of k?\n",
            "A)8\n",
            "B)9\n",
            "C)10\n",
            "D)11\n",
            "E)12\n",
            "Correct answer: A\n",
            "Question: Rs. 5600 is divided into three parts A, B and C. How much A is more than C if their ratio is 1/7:1/7:1/14?\n",
            "A)300\n",
            "B)992\n",
            "C)1120\n",
            "D)552\n",
            "E)312\n",
            "Correct answer: C\n",
            "Question: q is a positive integer and multiple of 2; p = 4^q, what is the remainder when p is divided by 10?\n",
            "A)10\n",
            "B)6\n",
            "C)4\n",
            "D)0\n",
            "E)It Cannot Be Determined\n",
            "Correct answer: B\n",
            "Batch 2:\n",
            "Question: Carl is facing very difficult financial times and can only pay the interest on a $10,000 loan he has taken. The bank charges him a quarterly compound rate of 4%. What is the approximate interest he pays annually?\n",
            "A)$1600\n",
            "B)$2000\n",
            "C)$2150\n",
            "D)$2500\n",
            "E)$12000\n",
            "Correct answer: A\n",
            "Question: Find the smallest number of five digits exactly divisible by 22,33,66 and 44.\n",
            "A)10101\n",
            "B)11000\n",
            "C)10110\n",
            "D)10111\n",
            "E)10100\n",
            "Correct answer: E\n",
            "Question: If (x^2 + 4x - 11)/5 ≤ x + 1, then x could be represented by which of the following?\n",
            "A)− 3 ≤ x ≤ 4\n",
            "B)− 4 ≤ x ≤ 3\n",
            "C)− 3 ≤ x ≤ 3\n",
            "D)− 4 ≤ x ≤ − 3\n",
            "E)3 ≤ x ≤ 4\n",
            "Correct answer: A\n",
            "Question: Mike took 5 mock tests before appearing for the GMAT. In each mock test he scored 10 points more than the previous mock test. If he scored 760 on the GMAT and his average score for the mocks and the GMAT was 716.67, what was the difference in the score of his last mock and his GMAT score?\n",
            "A)20\n",
            "B)32\n",
            "C)40\n",
            "D)50\n",
            "E)60\n",
            "Correct answer: B\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 定义自定义数据集类\n",
        "class QuestionDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item['question']\n",
        "        options = item['options']\n",
        "        combined_question = question\n",
        "        for option in options:\n",
        "            combined_question += f\"\\n{option}\"\n",
        "        answer = item['correct']\n",
        "        return combined_question, answer\n",
        "\n",
        "# 读取数据集\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "# 创建数据集\n",
        "data = data[:20]\n",
        "question_dataset = QuestionDataset(data)\n",
        "\n",
        "# 创建数据加载器\n",
        "data_loader = DataLoader(question_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# 查看数据集中的一些样本\n",
        "for i, (questions, answers) in enumerate(data_loader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    for question, answer in zip(questions, answers):\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Correct answer: {answer}\")\n",
        "    if i >= 1:  # 查看前两个批次\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1135864f-60bb-45f4-aff9-ba45144f4bf0",
      "metadata": {
        "id": "1135864f-60bb-45f4-aff9-ba45144f4bf0"
      },
      "outputs": [],
      "source": [
        "# 准确率计算函数\n",
        "def calculate_accuracy(predictions, references):\n",
        "    correct = 0\n",
        "    total = len(references)\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        if pred == ref:\n",
        "            correct += 1\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fd12bc96",
      "metadata": {
        "id": "fd12bc96"
      },
      "outputs": [],
      "source": [
        "def trim(s: str, i: int) -> str:\n",
        "    \"\"\"\n",
        "    extract the ith inference\n",
        "    input:\n",
        "        s: string, input\n",
        "        i: the number of inference you want to extract\n",
        "\n",
        "    output:\n",
        "        string, the ith sentence\n",
        "    \"\"\"\n",
        "\n",
        "    inference = s.split(\"Let's think step by step.\\n\")[2]\n",
        "    result = inference.split(f\"{i}. \")[1]\n",
        "    result = result.split(\"\\n\")[0]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e302be8b",
      "metadata": {
        "id": "e302be8b"
      },
      "outputs": [],
      "source": [
        "shot_question = \"question: What is the least value of x, So that 2x5475 is divisible by 9? \"\n",
        "shot_option = \"options: A)7, B)8, C)4, D)3, E)2\\n\"\n",
        "shot_prompt = \"Let's think step by step.\\n\"\n",
        "shot_reason_1 = \"1. The sum of the digits of the number is divisible by 9. Then the number is divisible by 9.\\n\"\n",
        "shot_reason_2 = \"2. We have 2 + x + 5 + 4 + 7 + 5 = 23 + x.\\n\"\n",
        "shot_reason_3 = \"3. The number after 23 and divisible by 9 is 27.\\n\"\n",
        "shot_reason_4 = \"4. Least value of x may be '4', so that the total 23 + 4 = 27.\\n\"\n",
        "shot_answer = \"5. Answer: C.\\n\"\n",
        "one_shot_example = shot_question + shot_option + shot_prompt + shot_reason_1 + shot_reason_2 + shot_reason_3 + shot_reason_4 + shot_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "48171bfb",
      "metadata": {
        "id": "48171bfb"
      },
      "outputs": [],
      "source": [
        "def extract_answer(input: str, mode: bool):\n",
        "    \"\"\"\n",
        "    extract answer from given input\n",
        "    if mode == True: extract letter\n",
        "    if mode == False: extract number\n",
        "    \"\"\"\n",
        "    if mode == False:\n",
        "        return re.findall(r'\\d+', input)[-1]\n",
        "    elif mode == True:\n",
        "        return re.findall(r'\\b[A-E]\\b', input)[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "36d4dd55",
      "metadata": {
        "id": "36d4dd55"
      },
      "outputs": [],
      "source": [
        "def multi_vote_answer(answers: list):\n",
        "    return max(set(answers), key=answers.count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e749782d",
      "metadata": {
        "id": "e749782d"
      },
      "outputs": [],
      "source": [
        "def select(background: str, infs: list) -> int:\n",
        "    \"\"\"\n",
        "    select one from infs\n",
        "    \"\"\"\n",
        "\n",
        "    hyper = {\n",
        "        \"do_sample\": False,\n",
        "        \"max_new_tokens\": 5,\n",
        "        \"num_beams\": 10,\n",
        "        \"num_return_sequences\": 1,\n",
        "        \"pad_token_id\": pad_token_id\n",
        "    }\n",
        "\n",
        "    prompt = \"Given background above, which of following is the next most reasonable inference? \"\n",
        "    for i in range(len(infs)):\n",
        "        prompt += f\"#{i+1}: {infs[i]}\\n\"\n",
        "\n",
        "    prompt += \"Answer: #\"\n",
        "    input = background + prompt\n",
        "    output = pipe(input, **hyper)\n",
        "    output = output[0]['generated_text']\n",
        "    ans = extract_answer(output, False)\n",
        "    ans = int(ans)\n",
        "    if (ans - 1) < 0 or ans >= len(infs):   # invalid selection\n",
        "        return -1\n",
        "\n",
        "    return ans - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a65b88ce",
      "metadata": {
        "id": "a65b88ce"
      },
      "outputs": [],
      "source": [
        "def inference(input: str, hyper_list) -> list:\n",
        "    \"\"\"\n",
        "    input: prompt + question\n",
        "\n",
        "    return: list of answer for each path\n",
        "    \"\"\"\n",
        "    i = 1\n",
        "    current_inf = input\n",
        "    infs = []\n",
        "    answers = []\n",
        "    delete = []\n",
        "\n",
        "    while i < 8 and len(answers) < 3:                                   # most 8 steps\n",
        "        for j in range(len(hyper_list)):\n",
        "            if j in delete:\n",
        "                continue\n",
        "            try:\n",
        "                output = pipe(current_inf, **(hyper_list[j]))     # infer in conservative, middle and creative mode and add them to infs\n",
        "                output = output[0]['generated_text']\n",
        "                inf = trim(output, i)\n",
        "                print(f\"Hyparams{j}:{inf}\")\n",
        "                if \"Answer\" in inf:                 # find answer, add to \"answers\" list and remove from hyper_list\n",
        "                    ans = extract_answer(inf, True)\n",
        "                    answers.append(ans)\n",
        "                    delete.append(j)\n",
        "                    #return ans\n",
        "                else:\n",
        "                    infs.append(inf)\n",
        "            except:\n",
        "                print(f\"Hyparams{j}: Error\")\n",
        "                delete.append(j)\n",
        "\n",
        "        if len(delete) == 3:\n",
        "            break\n",
        "\n",
        "        background = current_inf.split(shot_answer)[1]               # background doesn't contain shot example\n",
        "        choose = select(background, infs)\n",
        "        if choose == -1 or choose >= len(infs):                                            # if no valid answer, choose the first one\n",
        "            choose = 0\n",
        "\n",
        "        current_inf += f\"{i}. {infs[choose]}\\n\"                       # add the choosen inference to current inf\n",
        "        i += 1\n",
        "        infs.clear()\n",
        "        \n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30daba5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30daba5c",
        "outputId": "2cfa14af-3dc5-4ce4-9b89-21cb25daeee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "correct answer is E\n",
            "Hyparams0:The total distance of the trail is 43 km.\n",
            "Hyparams1:Friend P starts 15% faster than Friend Q.\n",
            "Hyparams2:They start at the opposite ends of the trail at the same time.\n",
            "Hyparams0:Friend P's rate is 15% faster than Friend Q's.\n",
            "Hyparams1:Friend P's rate is 15% faster than Friend Q's, so P's rate is 1.15 times Q's rate.\n",
            "Hyparams2:Friend P's rate is 15% faster than Friend Q's.\n",
            "Hyparams0:Friend P starts at the opposite end of the trail.\n",
            "Hyparams1:Let's say that Friend P has walked x km and Friend Q has walked (43 - x) km.\n",
            "Hyparams2:If Friend P has walked 43 km/15% = 28.28 km, then Friend Q will have walked 28.28 km/15% = 19.38 km.\n",
            "Hyparams0:Friend Q starts at the same end of the trail.\n",
            "Hyparams1:Friend P will have walked 43 km/15% = 29.17 km when he passes Friend Q.\n",
            "Hyparams2:Friend P will have walked 43/15 \\* 43 = 30 km when Friend P and Q pass each other.\n",
            "Hyparams0:When they pass each other, Friend P will have walked 43/1.15 = 37.9 km.\n",
            "Hyparams1:The time taken by Friend P to cover the total distance is 43/0.15 = 288/15 = 19.2 hours.\n",
            "Hyparams2:The total distance covered by Friend P is 15% more than the total distance covered by Friend Q.\n",
            "Hyparams0:Answer: D.\n",
            "Hyparams1:Answer: D.\n",
            "Hyparams2:Answer: A.\n",
            "answer: D\n",
            "Predicted Answer: D\n",
            "correct answer is E\n",
            "Hyparams0:The number must be divisible by 22.\n",
            "Hyparams1:We need a number with 5 digits, divisible by 22, 33, 66 and 44.\n",
            "Hyparams2:The sum of the digits of the number is divisible by 4. Then the number is divisible by 4.\n",
            "Hyparams0:We can divide 22 by 2 and 66 by 2, but not 33 and 44 by 2.\n",
            "Hyparams1:We can take any number and multiply it by 22, 33, 66 and 44 respectively.\n",
            "Hyparams2:We can use the fact that the sum of the digits of a number must be divisible by the sum of the digits of 11 (which is 33).\n",
            "Hyparams0:The number should have 5 digits, so we need to add 2 to each digit.\n",
            "Hyparams1:Therefore, the smallest such number is 22222 (22 x 2) + 44.\n",
            "Hyparams2:So, the number must have the digits 2 and 6.\n",
            "Hyparams0:The number should be divisible by 44, so we need the sum of the digits to be divisible by 4.\n",
            "Hyparams1:The number is 2 + 2 + 2 + 2 + 4 = 12.\n",
            "Hyparams2:The only combination possible is 22222, which is 11000.\n",
            "Hyparams0:The number should be divisible by 22, so we need the number to be divisible by 2.\n",
            "Hyparams1:The sum of the digits is 3 + 1 + 0 + 0 + 0 = 3, which is divisible by 4.\n",
            "Hyparams2:The number must be divisible by 22, 33 and 66.\n",
            "Hyparams0:The number should be divisible by 33, so we need the sum of the digits to be divisible by 3.\n",
            "Hyparams1:The number should be divisible by 33, so we need the sum of the digits to be divisible by 3.\n",
            "Hyparams2:The number should be divisible by 33, so we need the number to be divisible by 3.\n",
            "Hyparams0:The number should be divisible by 66, so we need the sum of the digits to be divisible by 6.\n",
            "Hyparams1:The number should be divisible by 66, so we need the number to be divisible by 6.\n"
          ]
        }
      ],
      "source": [
        "predictions = []\n",
        "for i, (questions, answers) in enumerate(data_loader):\n",
        "    questions = list(questions)\n",
        "    answers = list(answers)\n",
        "    for question, answer in zip(questions, answers):\n",
        "        try:\n",
        "            input = f\"{one_shot_example} question: {question}. {shot_prompt}\"\n",
        "            print(f\"correct answer is {answer}\")\n",
        "            answers = inference(input,copy.deepcopy(hyperparameters_list))\n",
        "            answer = multi_vote_answer(answers)\n",
        "            print(f\"answer: {answer}\")\n",
        "            if answer:\n",
        "                predictions.append(answer)\n",
        "                print(f\"Predicted Answer: {answer}\")\n",
        "            else:\n",
        "                predictions.append(None)  # 如果没有找到大写字母，则预测为空\n",
        "                print(\"Predicted Answer: None\")\n",
        "        except :\n",
        "            predictions.append(None)  # 如果没有找到大写字母，则预测为空\n",
        "            print(\"Predicted Answer: None\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d705654",
      "metadata": {
        "id": "5d705654",
        "outputId": "33380073-a8a2-4330-8d45-1093b6475453"
      },
      "outputs": [],
      "source": [
        "# 计算准确率\n",
        "accuracy = calculate_accuracy(predictions, answers)\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad13375c",
      "metadata": {
        "id": "ad13375c"
      },
      "outputs": [],
      "source": [
        "# 打印一些样本数据\n",
        "print(\"\\nSample Outputs:\")\n",
        "for q, a, p in zip(questions[:15], answers[:15], predictions[:15]):\n",
        "    print(f\"Question: {q}\")\n",
        "    print(f\"True Answer: {a}\")\n",
        "    print(f\"Predicted Answer: {p}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0043cccd25ea479c98ecf93605cc384e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f23a82d8a054641ae3017fe6d41eb41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b2241d4068b412fb4be01e63ea9058f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4131e626015e4850aefad8ec21560ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "506a2d455ab745a3a26cbec84634f66f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bc883da40f449a698a2136a98bfb57d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af1bbefab10e4c5c9cb7da6a3641d58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f23a82d8a054641ae3017fe6d41eb41",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0043cccd25ea479c98ecf93605cc384e",
            "value": 2
          }
        },
        "b10e37c29c5c446291cca30192fda169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9007005bb024056b8e879d34d7a1248",
              "IPY_MODEL_af1bbefab10e4c5c9cb7da6a3641d58c",
              "IPY_MODEL_d683fb0093cc4cd8abf62ab6e19a7068"
            ],
            "layout": "IPY_MODEL_9bc883da40f449a698a2136a98bfb57d"
          }
        },
        "b25fc54d63ed482b9eb9fb5aaf14890c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d683fb0093cc4cd8abf62ab6e19a7068": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b2241d4068b412fb4be01e63ea9058f",
            "placeholder": "​",
            "style": "IPY_MODEL_4131e626015e4850aefad8ec21560ca6",
            "value": " 2/2 [00:13&lt;00:00,  5.97s/it]"
          }
        },
        "e9007005bb024056b8e879d34d7a1248": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b25fc54d63ed482b9eb9fb5aaf14890c",
            "placeholder": "​",
            "style": "IPY_MODEL_506a2d455ab745a3a26cbec84634f66f",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
